{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Gradient Descent\n",
    "- Problem type: Regression with multiple features\n",
    "\n",
    "In this code, we will discuss multivariate gradient descent.\n",
    "\n",
    "### Objective\n",
    "See 3 different types of gradient descent\n",
    "\n",
    "- Batch gradient descent\n",
    "- Stochastic gradient descent\n",
    "- Mini batch gradient descent\n",
    "\n",
    "**Author**: Ashish Gupta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 3), (100,), numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y = make_regression(n_samples = 100, n_features=3, noise = 1000.0, n_informative=3, shuffle = True, random_state=3) \n",
    "X.shape,Y.shape, type(X), type(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.56195167,  1.09465308, -0.29382124],\n",
       "       [ 1.        , -1.15903647,  0.90831872,  1.50326195],\n",
       "       [ 1.        , -1.74411025,  0.28703516,  0.22575027],\n",
       "       [ 1.        , -0.52526465, -0.9064947 , -1.67563463],\n",
       "       [ 1.        ,  0.35081584, -1.41987131,  1.6265736 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing step\n",
    "# Our X is almost there, we just need to add 1s in front. X= [1 X]\n",
    "ones = np.ones((X.shape[0],1))\n",
    "ones.shape, X.shape\n",
    "X_appended = np.append(ones, X, axis = 1)\n",
    "X_appended[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= X_appended.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff96aa64f28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sns is a lot easier to plot. It uses exactly the same code except plt.plot changes to sns.lineplot or sns.scatterplot\n",
    "sns.scatterplot(x = X[:,2], y= Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large noise is added to see the distinct effect of stochastic, mini batch and batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def hypothesis(x,theta):\n",
    "    \n",
    "    # assumes x to be appended with array of 1s in the start\n",
    "    result = np.dot(theta,x.T)\n",
    "    return result\n",
    "\n",
    "def predictions(X,theta):\n",
    "    # stays the same\n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        pred = hypothesis(X[i],theta)\n",
    "        y_pred.append(pred)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return y_pred\n",
    "    \n",
    "def get_error(X,Y,theta):\n",
    "    # Least Square Error\n",
    "    e = 0\n",
    "    m = X.shape[0]\n",
    "    for i in range(m):\n",
    "        e += (hypothesis(X[i],theta)-Y[i])**2\n",
    "        \n",
    "    return e\n",
    "\n",
    "def getGradients(X,Y,theta):    \n",
    "    grads = np.zeros((X.shape[1]))\n",
    "    n = len(grads) # no. of features inc. bias\n",
    "    m = X.shape[0]\n",
    "    for i in range(m):\n",
    "        YPi = hypothesis(X[i],theta)\n",
    "        for grad_index in range(n):\n",
    "            grads[grad_index] += (YPi - Y[i])* X[i,grad_index]           \n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent\n",
    "\n",
    "You can refer to first answer in the link above for the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,Y,maxItr=100,learning_rate=0.001):\n",
    "    \n",
    "    # Write code here\n",
    "    num_features = None\n",
    "    theta = None\n",
    "    #\n",
    "    error = []\n",
    "    \n",
    "    for i in range(maxItr):\n",
    "        # Write code here\n",
    "        grad = None\n",
    "        #\n",
    "        e = get_error(X,Y,theta)\n",
    "        \n",
    "        # for loop for multi feature set\n",
    "        for i in range(num_features):\n",
    "            # Write code here\n",
    "            theta[i]= None\n",
    "            #\n",
    "            \n",
    "        error.append(e)\n",
    "        \n",
    "    return theta,error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOCHASTIC GRADIENT DESCENT (SGD)\n",
    "\n",
    "- In GD optimization, we compute the cost gradient based on the complete training set; hence, we sometimes also call it batch GD. In case of very large datasets, using GD can be quite costly since we are only taking a single step for one pass over the training set -- thus, the larger the training set, the slower our algorithm updates the weights and the longer it may take until it converges to the global cost minimum (note that the SSE cost function is convex).\n",
    "\n",
    "- In Stochastic Gradient Descent (SGD; sometimes also referred to as iterative or on-line GD), we don't accumulate the weight updates \n",
    "\n",
    "https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent\n",
    "\n",
    "You can refer to first answer in the link above for the algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochasticGradientDescent(X,Y,maxItr=100, learning_rate = 0.001):\n",
    "    \n",
    "    # Write code here\n",
    "    num_features = None\n",
    "    theta = None\n",
    "    #\n",
    "    \n",
    "    error = []\n",
    "    \n",
    "    graph_count=0\n",
    "    for i in range(maxItr):\n",
    "        for j in range(len(X)): # j is index of each training example\n",
    "            for k in range(len(theta)):\n",
    "                # Write code here  \n",
    "                theta[k]=None\n",
    "                #\n",
    "                \n",
    "                graph_count+=1\n",
    "                if graph_count%100 == 0:\n",
    "                    e=get_error(X,Y,theta)\n",
    "                    error.append(e)\n",
    "\n",
    "    return theta,error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MINI-BATCH GRADIENT DESCENT (MB-GD)\n",
    "\n",
    "Mini-Batch Gradient Descent (MB-GD) a compromise between batch GD and SGD. In MB-GD, we update the model based on smaller groups of training samples; instead of computing the gradient from 1 sample (SGD) or all n training samples (GD), we compute the gradient from 1 < k < n training samples (a common mini-batch size is k=50).\n",
    "MB-GD converges in fewer iterations than GD because we update the weights more frequently; however, MB-GD let's us utilize vectorized operation, which typically results in a computational performance gain over SGD.\n",
    "\n",
    "https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent\n",
    "\n",
    "You can refer to first answer in the link above for the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miniBatchGradientDescent(X,Y,maxItr=100, learning_rate = 0.001, batch_size = 50):\n",
    "    \n",
    "    # Write code here\n",
    "    num_features = None\n",
    "    theta = None\n",
    "    #\n",
    "    error = []\n",
    "    \n",
    "    for i in range(maxItr):\n",
    "        for batch in range(round(len(X)/(batch_size*1.0))+1): \n",
    "            for k in range(len(theta)):\n",
    "                # Write code here\n",
    "                grad = None\n",
    "                theta[k]= None\n",
    "                #\n",
    "                \n",
    "                e=get_error(X,Y,theta)\n",
    "                error.append(e)\n",
    "\n",
    "    return theta,error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "theta, error= gradientDescent(X,Y,50,0.001)\n",
    "sns.scatterplot(data = np.array(error))\n",
    "plt.title(\"Batch Gradient Descent\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Error\")\n",
    "\n",
    "plt.figure(2)\n",
    "theta_stochastic, error_stochastic= stochasticGradientDescent(X,Y,50,0.001)\n",
    "sns.lineplot(data = np.array(error_stochastic))\n",
    "plt.title(\"Stochastic Gradient Descent\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Error\")\n",
    "                                      \n",
    "plt.figure(3)\n",
    "theta_stochastic, error_stochastic= miniBatchGradientDescent(X,Y,50,0.001, batch_size=50)\n",
    "sns.lineplot(data = np.array(error_stochastic))\n",
    "plt.title(\"Mini Batch Gradient Descent\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Error\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error is of the order e^7. This is because of noise = 1000, the huge error is natural. It was done to show the zig zag nature of cost at convergence. Stochastic gradient descent is pseudo convex and it almost always converges ( You never have to worry about whether it will converge or not)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
